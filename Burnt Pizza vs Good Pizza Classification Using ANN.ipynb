{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Name: Burnt Pizza vs Good Pizza Classification Using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A famous 30 year old pizza brand which has outlets in more than 90 countries started home\n",
    "delivery services a couple of years ago and the business has grown much faster than expected.\n",
    "However, outlet vendors are very much disappointed with few customers for their cheating\n",
    "activities. This is because vendors, shockingly, came to know that few customers after receiving the\n",
    "delivery are raising tickets for refund in the name of burnt pizzas. Even though customers received\n",
    "a good pizza but still few customers are trying to cheat vendors. To overcome this issue, Franchise\n",
    "has come up with an idea to integrate a pizza detection model in their application where customers\n",
    "can upload images for the burnt pizzas delivered. For example, if I have received a burnt pizza then\n",
    "I can upload a couple of images of the pizza to the application and it will classify the pizza as burnt\n",
    "or good in order to process my refund ticket.\n",
    "\n",
    "**Goal:** You are hired as Deep Learning Engineer by a famous pizza franchise. You are asked to build\n",
    "a model where it accepts the images of pizza and detects as burnt pizza or good pizza.\n",
    "\n",
    "**Constraints:** You should be using only ANN and shouldnâ€™t be using CNN or any other rule based\n",
    "model to generate results.\n",
    "\n",
    "**Data Description:** Data is in the form of images collected from multiple sources of the internet.\n",
    "    \n",
    "**Provided Files:**\n",
    "**Train set:** Train set is divided into burnt pizza and good pizza categories. While training the\n",
    "model you can label images of good pizza as 1 and burnt pizza as 0.\n",
    "\n",
    "**Test:** Test set contains mixed images of both burnt pizzas and good pizzas.\n",
    "    \n",
    "**Instructions:**\n",
    "1. Train set should be used to feed the model.\n",
    "2. Test set should be used to predict labels for test data.\n",
    "\n",
    "**Evaluation Criteria:** The evaluation metric for this problem statement is the Accuracy score\n",
    "where each image label is matched with the actual image label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "    \n",
    "1. Importing (or installing) Tenosrflow, Keras and other packages on your system\n",
    "2. Loading data from disk\n",
    "3. Creating your training and testing splits\n",
    "4. Data Preprocessing \n",
    "5. Defining your tensorflow ANN model architecture\n",
    "6. Compiling your tensorflow ANN model\n",
    "7. Training your model on your training data\n",
    "8. Evaluating your model on your test data\n",
    "9. Generate Plots for accuracy and validation loss\n",
    "10. Saving The train model\n",
    "11. Making predictions using your trained tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing all the packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix , accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading data from disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "time1=time.time()    # to measure time taken\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "classes=[\"Good_pizza\",\"Burnt_pizza\"]\n",
    "\n",
    "#grab the image path and randomly shuffle them\n",
    "imagePaths = sorted(list(paths.list_images(\"train\")))\n",
    "random.seed(SEED)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "#progress bar\n",
    "\n",
    "with tqdm(total=len(imagePaths)) as pbar:\n",
    "    \n",
    "    #loop over the input images\n",
    "    for imagePath in imagePaths:\n",
    "        #load the image, resize the image to be 32*32 pixel (ignoring aspect ratio),\n",
    "        #flatten the 32*32*3 =3072 pixel image into a list, and store image in data list\n",
    "        image =cv2.imread(imagePath)\n",
    "        image = cv2.resize(image,(32,32)).flatten()\n",
    "        data.append(image)\n",
    "        \n",
    "        #Extract the class label from the image path and update the label list\n",
    "        label=imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        label = 1 if label == \"Burnt_pizza\" else 0\n",
    "        labels.append(label)\n",
    "        \n",
    "        #update the progressbar\n",
    "        pbar.update(1)\n",
    "\n",
    "#scale the raw pixel intensities to the range [0,1]\n",
    "data=np.array(data,dtype=\"float\")/255.0\n",
    "labels=np.array(labels)\n",
    "\n",
    "print(\"Time Taken: {:.1f} seconds\".format(time.time()-time1)) # to measure the time taken\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Images: \",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample data for first image\n",
    "print(\"Sample image: {}\".format(data[0]))\n",
    "print(\"no of features/pixels values: {}\".format(len(data[0]))) #32*32*3=3072\n",
    "print(\"label:{}\".format(classes[labels[0]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Training and Testing Splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition the data into 80% training and 20 % validation\n",
    "(trainX,testX,trainY,testY)= train_test_split(data,labels,test_size=0.2,random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converst the Labels from integers/categories to vectors\n",
    "\n",
    "trainY = to_categorical(trainY,num_classes=2) #fit_transform = find all unique class labels + transform into one hot encoded labels\n",
    "testY = to_categorical(testY, num_classes=2) # transform = perform the one-hot encoding (unique class labels already found)\n",
    "\n",
    "# [0,1] Goood_Pizza\n",
    "# [1,0] Burn_Pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = (trainX[0]*255).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_image.reshape(32,32,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Architecture for ANN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the 3072-1024-512-1 architecture using keras\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#input layer 3072 as there are 32*32*3 = 3072 pixels in a flattern input image\n",
    "#first hidden Layer has 1024 nodes\n",
    "model.add(Dense(units=1024,input_shape = (3072,),kernel_initializer=\"uniform\",activation='relu'))\n",
    "\n",
    "#droput for second layer\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "#second hidden layer has 512 nodes\n",
    "model.add(Dense(units=512,kernel_initializer='uniform',activation='relu'))\n",
    "\n",
    "\n",
    "#output layer with number of possible class labels\n",
    "model.add(Dense(units=2,kernel_initializer='uniform',activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling Tensorflow ANN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize our initial learning rate and epochs to train for\n",
    "INIT_LR = 0.01\n",
    "EPOCHS = 50\n",
    "\n",
    "#compile the model using SGD as our optimizer and categorical cross_entropy loss\n",
    "# we have use binary_crossentropy for 2-class classification\n",
    "print(\"[INFO] compiling network...\")\n",
    "opt= SGD(lr=INIT_LR) #stochastic Gradient Descent (SGD) optimizer\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Model on Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the neural network on training data set\n",
    "#batch_size (32) controls the size of each group of data to pass trough netwaork\n",
    "\n",
    "time1 = time.time()   # to measure time taken\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=EPOCHS, batch_size=32)\n",
    "print('Time taken: {:.1f} seconds'.format(time.time() - time1))   # to measure time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating model on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "pred_prob=model.predict(testX,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9.95466590e-01, 4.53341054e-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert testY and y_pred into 1's and 0 for classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note Burn_pizza --> 1 and forest --> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = [np.argmax(i) for i in testY]\n",
    "pred_y = [np.argmax(i) for i in pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_metrix(y_true, y_pred,classes,\n",
    "                         normalize=False,\n",
    "                         title='Confusion Matrix',\n",
    "                         cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Objective\n",
    "    ----------\n",
    "    plot confussion matrix, classification report and accuracy score\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "\n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a classifier.\n",
    "    \n",
    "    classes : list\n",
    "        List of labels to index the matrix\n",
    "        \n",
    "    title : title for matrix\n",
    "    cmap : colormap for matrix \n",
    "    \n",
    "    returns \n",
    "    ----------\n",
    "   all accruacy matrix \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    \n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized Confusion Matrix\")\n",
    "    else:\n",
    "        print(\"Confusion Matrix, Without Normalisation\")\n",
    "\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes,rotation=35)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() /2.\n",
    "    \n",
    "    for i , j in itertools.product(range(cm.shape[0]), range(cm.shape[0])):\n",
    "        plt.text(j, i, format(cm[i,j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print('Classification report')\n",
    "    print(classification_report(y_true,y_pred))\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    acc= accuracy_score(y_true,y_pred)\n",
    "    print(\"Accuracy of the model: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_metrix(test_y,pred_y,classes=[\"Burn_Pizza\",\"Good_Pizza\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Plots for Accuracy and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation loss\n",
    "N=np.arange(0,EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize = [10,8])\n",
    "plt.plot(N,H.history[\"accuracy\"],label=\"Train Accuracy\")\n",
    "plt.plot(N,H.history[\"val_accuracy\"],label=\"Validation Accuracy\")\n",
    "plt.title(\"ANN: Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch #\",weight=\"bold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model and label binarizer to disk\n",
    "print(\"[INFO] serializing network and label binarizer..\")\n",
    "model.save(\"model_ANN.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making Predictions using your trained tensorflow model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import cv2\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "print(\"[INFO] loading network and ..\")\n",
    "model = load_model(\"model_ANN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img):\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    plt.grid(b=None)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image and resize it to the target spatial dimensions\n",
    "width = 32\n",
    "height = 32\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "testImagePaths = sorted(list(paths.list_images('test')))   # test data folder with random images\n",
    "\n",
    "\n",
    "# progress bar \n",
    "with tqdm(total=len(testImagePaths)) as pbar:\n",
    "    \n",
    "    for imagePath in testImagePaths:\n",
    "        image = cv2.imread(imagePath)\n",
    "        output = image.copy()\n",
    "        image = cv2.resize(image, (width, height))\n",
    "\n",
    "        # scale the pixel values to [0, 1]\n",
    "        image = image.astype(\"float\") / 255.0\n",
    "\n",
    "        # for a simple fully-connected network, flatten the image\n",
    "        image = image.flatten()\n",
    "        image = image.reshape((1, image.shape[0]))\n",
    "\n",
    "\n",
    "        # make a prediction on the image\n",
    "        preds = model.predict(image)\n",
    "\n",
    "        # find the class label index with the largest corresponding probability\n",
    "        i = preds.argmax(axis=1)[0]\n",
    "        label = classes[i]\n",
    "        \n",
    "        label = \"{}: {:.2f}%\".format(label, preds[0][i] * 100)\n",
    "\n",
    "        \n",
    "        output = imutils.resize(output, width=400)\n",
    "        cv2.putText(output, label, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # convert img to rgb format and display in noteboo\n",
    "        img = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "        display_img(img)\n",
    "\n",
    "#         print(\"############################\")\n",
    "#         print(\"image: {}\".format(os.path.split(imagePath)[-1]))\n",
    "#         print(\"predicted label: {}\".format(label))\n",
    "#         print(\"Confidence: {}\".format(preds[0][i]))\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    \n",
    "    image = cv2.resize(image, (32, 32))\n",
    "\n",
    "    # scale the pixel values to [0, 1]\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "\n",
    "    # for a simple fully-connected network, flatten the image\n",
    "    image = image.flatten()\n",
    "    image = image.reshape((1, image.shape[0]))\n",
    "\n",
    "    # make a prediction on the image\n",
    "    preds = model.predict(image).flatten()\n",
    "    result = dict()\n",
    "    result[\"Good_Pizza\"] = round(float(list(preds)[0]), 3)\n",
    "    result[\"Burnt_Pizza\"] = round(float(list(preds)[1]), 3)\n",
    "    print(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "im = gr.inputs.Image(shape=(32,32))\n",
    "label = gr.outputs.Label(num_top_classes=2)\n",
    "\n",
    "gr.Interface(fn=predict_image, inputs=im, outputs=label, capture_session=True, title=\"Check Pizza Burn or not\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
